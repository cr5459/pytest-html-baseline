Metadata-Version: 2.4
Name: pytest-html-baseline
Version: 0.1.0
Summary: Pytest plugin adding baseline comparison (failures, flaky, slowness) integrated with pytest-html and CI gating
Author: pytest-html-baseline maintainers
License: MIT
Keywords: pytest,plugin,html,baseline,regression,ci
Classifier: Framework :: Pytest
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Operating System :: OS Independent
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development :: Testing
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pytest>=7.0
Provides-Extra: html
Requires-Dist: pytest-html>=4.0; extra == "html"
Provides-Extra: test
Requires-Dist: pytest>=7.0; extra == "test"
Requires-Dist: pytest-html>=4.0; extra == "test"
Requires-Dist: hypothesis>=6.0; extra == "test"
Provides-Extra: dev
Requires-Dist: psutil>=5.9; extra == "dev"
Requires-Dist: hypothesis>=6.0; extra == "dev"
Requires-Dist: pytest-html>=4.0; extra == "dev"
Requires-Dist: pytest>=7.0; extra == "dev"
Dynamic: license-file

# pytest-html-baseline

Baseline comparison addon for pytest + pytest-html: detect new failures, vanished failures, flaky suspects, and performance regressions between test runs. Produces an HTML panel (when pytest-html is installed) and machine-readable JSON for CI gating.

## Features
- Capture a snapshot (baseline) of test outcomes & durations.
- Diff subsequent runs against the baseline: new failures, vanished failures, flaky suspects, slower tests.
- Threshold-based slowness detection (ratio + absolute) with env/CLI overrides.
- JSON diff output for automation, optional HTML summary panel.
- CI failure gating on regression types (new failures, slower tests, any).
- O(n) diff with low allocations (50k test pairs < 250ms / < 25MB target).
- Property-based invariants ensuring correctness.

## Install
```
pip install pytest-html-baseline
# optional (HTML panel):
pip install pytest-html-baseline[html]
```

## Quickstart
First run (create baseline):
```
pytest --html=report1.html --self-contained-html \
			 --html-save-baseline .artifacts/base.json
```

Second run (compare & update):
```
pytest --html=report2.html --self-contained-html \
			 --html-baseline .artifacts/base.json \
			 --html-save-baseline .artifacts/base.json \
			 --html-fail-on new-failures \
			 --html-slower-threshold-ratio 1.30 \
			 --html-slower-threshold-abs 0.20 \
			 --html-diff-json .artifacts/diff.json
```

## CLI Options
--html-save-baseline PATH : write snapshot after run
--html-baseline PATH       : load snapshot to diff
--html-slower-threshold-ratio FLOAT (default 1.30 / env HTML_SLOWER_RATIO)
--html-slower-threshold-abs FLOAT seconds (default 0.20 / env HTML_SLOWER_ABS)
--html-min-count INT (default 0 / env HTML_MIN_COUNT)
--html-fail-on {new-failures,slower,any} (default new-failures / env HTML_FAIL_ON)
--html-diff-json PATH : write diff JSON

## JSON Snapshot Format
Compact single object with version, timestamp, collected count, tests array (id, outcome, duration, sig).

## CI Example (GitHub Actions)
```yaml
name: tests
on: [push, pull_request]
jobs:
	test:
		runs-on: ubuntu-latest
		steps:
			- uses: actions/checkout@v4
			- uses: actions/setup-python@v5
				with:
					python-version: '3.12'
			- run: pip install .[html,test]
			- run: pytest --html-save-baseline baseline.json || true
			- run: pytest --html-baseline baseline.json --html-save-baseline baseline.json --html-diff-json diff.json --html-fail-on any
```

## Performance
Targets: 50k test diff < 250ms wall, < 25MB RSS.
Bench scripts under bench/ (run in smoke mode in CI). Fill in real numbers after local run.

## Design Notes
O(n) dict-based diff, minimal object churn, deterministic ordering in outputs for stability. Property-based tests validate invariants (partition, monotonicity, idempotence).

## License
MIT
